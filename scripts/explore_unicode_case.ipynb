{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unicode Normalization and Search Anchor Analysis\n",
    "\n",
    "This notebook explores Unicode case folding and normalization properties to identify optimal \"anchor points\" for case-insensitive and normalization-insensitive string search algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install -q tabulate 2>/dev/null || curl -sS https://bootstrap.pypa.io/get-pip.py | python && python -m pip install -q tabulate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Before we start, a small reminder on Unicode.\n",
    "Unicode is a versioned standard.\n",
    "In 2025, the latest version is Unicode 17.0.\n",
    "It defines over a million code points, of which around 150,000 are assigned characters.\n",
    "Some of them belong to \"bicameral\" scripts (like Latin, Greek, Cyrillic) that have distinct uppercase and lowercase forms.\n",
    "Others belong to \"unicameral\" scripts (like Chinese, Japanese, Korean, Arabic) that do not have case distinctions.\n",
    "It doesn't, however, mean that there are no different ways to represent the same character in the same script.\n",
    "So \"case folding\" and \"normalization\" are two different concepts.\n",
    "We will explore both in this notebook.\n",
    "\n",
    "Unicode also doesn't require UTF-8 encoding, but UTF-8 is the most popular encoding on the web and in modern applications and the one we will focus on in StringZilla.\n",
    "In UTF-8, each code point is represented by one, two, three, or four bytes.\n",
    "A folded or normalized character can map to a sequence of multiple code points, and each of those code points can have a different length representation in UTF-8.\n",
    "That's why, in the absolute majority of modern text-processing applications full Unicode processing is disabled.\n",
    "\n",
    "Typically, when people perform case-insensitive search, they either:\n",
    "\n",
    "1. Use simple ASCII case folding (A-Z to a-z), ignoring all other characters.\n",
    "2. Use pretty much the only major library that supports full Unicode case folding and normalization, ICU (International Components for Unicode).\n",
    "\n",
    "The first is clearly insufficient, and the second is quite heavy and works at a character level, making SIMD optimizations difficult.\n",
    "This notebook will focus on more SIMD-vectorizable ideas.\n",
    "\n",
    "To start, let's pull the most recent Unicode Character Database (UCD) files from the Unicode website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from collections import Counter\n",
    "import unicodedata\n",
    "from tabulate import tabulate\n",
    "\n",
    "# Import shared Unicode data loading functions\n",
    "sys.path.insert(0, \".\")\n",
    "from test_helpers import (\n",
    "    UNICODE_VERSION,\n",
    "    get_all_codepoints,\n",
    "    get_case_folding_rules_as_codepoints,\n",
    ")\n",
    "\n",
    "# UTF-8 byte boundaries\n",
    "UTF8_1BYTE_MAX = 0x7F  # 127 - ASCII range\n",
    "UTF8_2BYTE_MAX = 0x7FF  # 2047\n",
    "UTF8_3BYTE_MAX = 0xFFFF  # 65535\n",
    "\n",
    "\n",
    "def utf8_hex(text):\n",
    "    \"\"\"Return UTF-8 hex byte sequence for a string.\"\"\"\n",
    "    return \" \".join(f\"0x{b:02X}\" for b in text.encode(\"utf-8\"))\n",
    "\n",
    "\n",
    "print(f\"Using Unicode version: {UNICODE_VERSION}\")\n",
    "all_codepoints = get_all_codepoints(UNICODE_VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[The highest allowed code point in Unicode is `0x10FFFF` or \"U+10FFFF\"](https://stackoverflow.com/questions/52203351/why-is-unicode-restricted-to-0x10ffff), but it doesn't mean that all code points up to that value are assigned.\n",
    "\n",
    "- Planes 15-16 (U+F0000 to U+10FFFF) are reserved for \"Private Use Area\" and do not contain assigned characters.\n",
    "- Most of plane 14 (U+E0000 to U+E0FFF) is reserved for \"Supplementary Special-purpose Plane\" and contains very few assigned characters.\n",
    "- Many code points in other planes are also unassigned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total assigned codepoints: {len(all_codepoints):,}\")\n",
    "print(f\"Highest assigned codepoint: {all_codepoints[-1]:,}\")\n",
    "print(f\"Highest possible codepoint: {0x10FFFF:,}\")\n",
    "print(f\"Range density: {len(all_codepoints) / (all_codepoints[-1] + 1):.6%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unicode Case Folding Analysis\n",
    "\n",
    "### Direct Folding Targets\n",
    "\n",
    "Case folding maps characters to a \"folded\" form for case-insensitive comparisons.\n",
    "This is more comprehensive than simple lowercasing - it handles special cases like German ß → ss.\n",
    "The very first thing we are interested in is: how often each codepoint becomes a folding target for other characters?\n",
    "\n",
    "The reason we are curious about this is that in simple cases, like the Russian letter \"А\" (A) and \"а\" (a), both fold to the same codepoint U+0430 (Cyrillic small letter a).\n",
    "So when scanning for exact case-insensitive matches, we can just compare each 2-byte UTF-8 slice against just 2 possible values: 0xD090 (U+0410) and 0xD0B0 (U+0430), without actually performing any case folding.\n",
    "The easiest way to solve the problem is to avoid it after all!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case_folds = get_case_folding_rules_as_codepoints(UNICODE_VERSION)\n",
    "print(f\"Total case folding rules: {len(case_folds):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_frequency = Counter()\n",
    "\n",
    "for source_codepoint, target_codepoints in case_folds.items():\n",
    "    for target_codepoint in target_codepoints:\n",
    "        target_frequency[target_codepoint] += 1\n",
    "\n",
    "print(f\"Total folding rules: {sum(target_frequency.values()):,}\")\n",
    "print(f\"Unique target codepoints: {len(target_frequency):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's display the most common folding targets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "for codepoint, frequency in target_frequency.most_common():\n",
    "    try:\n",
    "        character = chr(codepoint)\n",
    "        name = unicodedata.name(character, \"\")\n",
    "        hex_bytes = utf8_hex(character)\n",
    "    except (ValueError, OverflowError):\n",
    "        character = \"?\"\n",
    "        name = \"\"\n",
    "        hex_bytes = \"?\"\n",
    "    rows.append(\n",
    "        {\n",
    "            \"Codepoint\": f\"U+{codepoint:04X}\",\n",
    "            \"Char\": character,\n",
    "            \"UTF-8\": hex_bytes,\n",
    "            \"Freq\": frequency,\n",
    "            \"Name\": name,\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(tabulate(rows, headers=\"keys\", tablefmt=\"github\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This suggests that the \"GREEK SMALL LETTER IOTA\" (U+03B9) is the most common folding target, being the folded form of 71 different codepoints.\n",
    "The reason for this is historical.\n",
    "Ancient Greek had a grammatical feature called the \"iota subscript\" where iota was written as a small subscript beneath vowels (α, η, ω) to indicate certain grammatical forms (dative case, etc.).\n",
    "When case-folding, these decompose and the subscript iota becomes a regular lowercase iota:\n",
    "\n",
    "- ᾳ (alpha with ypogegrammeni) → αι\n",
    "- ῃ (eta with ypogegrammeni) → ηι\n",
    "- ῳ (omega with ypogegrammeni) → ωι\n",
    "\n",
    "More importantly, at this point we see that `'f'`, `'s'`, `'i'`, `'t'` are the most common direct single-byte UTF-8 folding targets.\n",
    "Each is the target of at least 4 different codepoints.\n",
    "But that doesn't tell the whole story!\n",
    "\n",
    "### Otherwise Ambiguous Folding Targets\n",
    "\n",
    "Oftentimes, a character is only one of many characters in the produced folding result.\n",
    "\n",
    "- `'ﬀ'` → `\"ff\"` - 3-byte codepoint mapping into 2x 1-byte codepoints\n",
    "- `'ﬁ'` → `\"fi\"` - 3-byte codepoint mapping into 2x 1-byte codepoints\n",
    "- `'ﬂ'` → `\"fl\"` - 3-byte codepoint mapping into 2x 1-byte codepoints\n",
    "- `'ﬃ'` → `\"ffi\"` - 3-byte codepoint mapping into 3x 1-byte codepoints\n",
    "- `'ﬄ'` → `\"ffl\"` - 3-byte codepoint mapping into 3x 1-byte codepoints\n",
    "\n",
    "Let's account for those as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "direct_target_frequency = Counter()  # Codepoint is the ONLY target of a folding\n",
    "partial_target_frequency = Counter()  # Codepoint is ONE OF multiple targets in a folding\n",
    "\n",
    "for source_codepoint, target_codepoints in case_folds.items():\n",
    "    if len(target_codepoints) == 1:\n",
    "        # Direct 1:1 folding (e.g., 'A' → 'a')\n",
    "        direct_target_frequency[target_codepoints[0]] += 1\n",
    "    else:\n",
    "        # Multi-codepoint expansion (e.g., 'ﬁ' → 'f', 'i')\n",
    "        for target_codepoint in target_codepoints:\n",
    "            partial_target_frequency[target_codepoint] += 1\n",
    "\n",
    "# Some codepoints may be both direct AND partial targets\n",
    "both_targets = set(direct_target_frequency.keys()) & set(partial_target_frequency.keys())\n",
    "\n",
    "print(f\"Total folding rules: {len(case_folds):,}\")\n",
    "print(f\"  - Direct 1:1 foldings: {sum(1 for t in case_folds.values() if len(t) == 1):,}\")\n",
    "print(f\"  - Multi-codepoint expansions: {sum(1 for t in case_folds.values() if len(t) > 1):,}\")\n",
    "print()\n",
    "print(f\"Unique target codepoints:\")\n",
    "print(f\"  - Only direct targets: {len(direct_target_frequency - partial_target_frequency):,}\")\n",
    "print(f\"  - Only partial targets: {len(partial_target_frequency - direct_target_frequency):,}\")\n",
    "print(f\"  - Both direct AND partial: {len(both_targets):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following table differentiates complete and partial folding targets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "for codepoint, partial_frequency in partial_target_frequency.most_common():\n",
    "    try:\n",
    "        character = chr(codepoint)\n",
    "        hex_bytes = utf8_hex(character)\n",
    "    except (ValueError, OverflowError):\n",
    "        character = \"?\"\n",
    "        hex_bytes = \"?\"\n",
    "\n",
    "    direct_frequency = direct_target_frequency.get(codepoint, 0)\n",
    "\n",
    "    # Find an example expansion containing this codepoint\n",
    "    example = \"\"\n",
    "    example_hex = \"\"\n",
    "    for source_codepoint, target_codepoints in case_folds.items():\n",
    "        if len(target_codepoints) > 1 and codepoint in target_codepoints:\n",
    "            try:\n",
    "                source_character = chr(source_codepoint)\n",
    "                target_string = \"\".join(chr(c) for c in target_codepoints)\n",
    "                example = f\"'{source_character}' → \\\"{target_string}\\\"\"\n",
    "                example_hex = f\"{utf8_hex(source_character)} → {utf8_hex(target_string)}\"\n",
    "            except (ValueError, OverflowError):\n",
    "                example = f\"U+{source_codepoint:04X} → {target_codepoints}\"\n",
    "                example_hex = \"\"\n",
    "            break\n",
    "\n",
    "    rows.append(\n",
    "        {\n",
    "            \"Codepoint\": f\"U+{codepoint:04X}\",\n",
    "            \"Char\": character,\n",
    "            \"UTF-8\": hex_bytes,\n",
    "            \"Partial\": partial_frequency,\n",
    "            \"Direct\": direct_frequency,\n",
    "            \"Example\": example,\n",
    "            \"Example Hex\": example_hex,\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(tabulate(rows, headers=\"keys\", tablefmt=\"github\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Safe Single-byte Folding Anchors\n",
    "\n",
    "Of all those characters, we are most interested in the codepoints representable in just 1 byte in UTF-8, as we can process 64 of them in a `ZMM` register at once.\n",
    "Those are the boring ASCII letters.\n",
    "But we can't just apply traditional SIMD ASCII case-insensitive search techniques like:\n",
    "\n",
    "```c\n",
    "__m512i lower_mask = _mm512_set1_epi8(0x20);\n",
    "__m512i input_chunk = _mm512_loadu_si512(input_ptr);\n",
    "__m512i folded_chunk = _mm512_or_si512(input_chunk, lower_mask);\n",
    "```\n",
    "\n",
    "If the needle contains an `'f'` and the haystack contains an `'ﬃ'`, we would miss the match.\n",
    "So we must know, which of the single-byte codepoints are folding targets of multiple codepoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ascii_targets = {}\n",
    "for codepoint in range(UTF8_1BYTE_MAX + 1):\n",
    "    direct = direct_target_frequency.get(codepoint, 0)\n",
    "    partial = partial_target_frequency.get(codepoint, 0)\n",
    "    total = direct + partial\n",
    "    if total > 0:\n",
    "        ascii_targets[codepoint] = {\"direct\": direct, \"partial\": partial, \"total\": total}\n",
    "\n",
    "# Separate into \"safe\" (exactly 1 source) vs \"ambiguous\" (multiple sources)\n",
    "safe_ascii = {codepoint: info for codepoint, info in ascii_targets.items() if info[\"total\"] == 1}\n",
    "ambiguous_ascii = {codepoint: info for codepoint, info in ascii_targets.items() if info[\"total\"] > 1}\n",
    "\n",
    "print(f\"Total ASCII targets: {len(ascii_targets)}\")\n",
    "print(f\"  - Safe (exactly 1 source): {len(safe_ascii)}\")\n",
    "print(f\"  - Ambiguous (multiple sources): {len(ambiguous_ascii)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following table shows safe ASCII targets that can use simple SIMD case folding (each has exactly one source):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "safe_rows = []\n",
    "for codepoint in sorted(safe_ascii.keys()):\n",
    "    character = chr(codepoint)\n",
    "    for source_codepoint, target_codepoints in case_folds.items():\n",
    "        if codepoint in target_codepoints:\n",
    "            source_character = chr(source_codepoint)\n",
    "            safe_rows.append(\n",
    "                {\n",
    "                    \"Target\": f\"'{character}' (U+{codepoint:04X})\",\n",
    "                    \"Target Hex\": utf8_hex(character),\n",
    "                    \"Source\": f\"'{source_character}' (U+{source_codepoint:04X})\",\n",
    "                    \"Source Hex\": utf8_hex(source_character),\n",
    "                }\n",
    "            )\n",
    "            break\n",
    "\n",
    "print(tabulate(safe_rows, headers=\"keys\", tablefmt=\"github\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following table shows ambiguous ASCII targets that need special handling in SIMD (each has multiple sources):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ambiguous_rows = []\n",
    "for codepoint in sorted(ambiguous_ascii.keys()):\n",
    "    character = chr(codepoint)\n",
    "    info = ambiguous_ascii[codepoint]\n",
    "\n",
    "    # Find all sources\n",
    "    sources = []\n",
    "    for source_codepoint, target_codepoints in case_folds.items():\n",
    "        if codepoint in target_codepoints:\n",
    "            try:\n",
    "                source_character = chr(source_codepoint)\n",
    "                source_hex = utf8_hex(source_character)\n",
    "                if len(target_codepoints) == 1:\n",
    "                    sources.append(f\"'{source_character}' ({source_hex})\")\n",
    "                else:\n",
    "                    target_string = \"\".join(chr(c) for c in target_codepoints)\n",
    "                    target_hex = utf8_hex(target_string)\n",
    "                    sources.append(f\"'{source_character}'→\\\"{target_string}\\\" ({source_hex}→{target_hex})\")\n",
    "            except:\n",
    "                sources.append(f\"U+{source_codepoint:04X}\")\n",
    "\n",
    "    sources_string = \", \".join(sources[:4])\n",
    "    if len(sources) > 4:\n",
    "        sources_string += f\" (+{len(sources)-4} more)\"\n",
    "\n",
    "    ambiguous_rows.append(\n",
    "        {\n",
    "            \"Char\": f\"'{character}'\",\n",
    "            \"Hex\": utf8_hex(character),\n",
    "            \"Direct\": info[\"direct\"],\n",
    "            \"Partial\": info[\"partial\"],\n",
    "            \"Total\": info[\"total\"],\n",
    "            \"Sources\": sources_string,\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(tabulate(ambiguous_rows, headers=\"keys\", tablefmt=\"github\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, even \"ambiguous\" ASCII characters can be contextually safe based on what follows them in the needle.\n",
    "For example, `'f'` is ambiguous because of ligatures like `'ﬁ'` → `\"fi\"`.\n",
    "But if the needle contains `\"fog\"`, the `'f'` is safe because no ligature expands to `\"fo...\"`.\n",
    "The following analysis identifies when each ambiguous character becomes safe based on its context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contextual_safety = {}\n",
    "\n",
    "for codepoint in ambiguous_ascii.keys():\n",
    "    char = chr(codepoint)\n",
    "    dangerous_following = set()\n",
    "    dangerous_preceding = set()\n",
    "    ligature_examples = []\n",
    "\n",
    "    # Find all multi-codepoint expansions that include this character\n",
    "    for source_codepoint, target_codepoints in case_folds.items():\n",
    "        if len(target_codepoints) > 1:  # Multi-codepoint expansion\n",
    "            expansion = \"\".join(chr(c) for c in target_codepoints)\n",
    "\n",
    "            # Find all positions where our character appears\n",
    "            for pos, c in enumerate(expansion):\n",
    "                if ord(c) == codepoint:\n",
    "                    source_char = chr(source_codepoint)\n",
    "\n",
    "                    # If not the last character, next char is \"dangerous following\"\n",
    "                    if pos < len(expansion) - 1:\n",
    "                        next_char = expansion[pos + 1]\n",
    "                        dangerous_following.add(next_char)\n",
    "                        if len(ligature_examples) < 3:\n",
    "                            ligature_examples.append(f\"'{source_char}'→\\\"{expansion}\\\"\")\n",
    "\n",
    "                    # If not the first character, prev char is \"dangerous preceding\"\n",
    "                    if pos > 0:\n",
    "                        prev_char = expansion[pos - 1]\n",
    "                        dangerous_preceding.add(prev_char)\n",
    "\n",
    "    if dangerous_following or dangerous_preceding:\n",
    "        contextual_safety[char] = {\n",
    "            \"dangerous_following\": dangerous_following,\n",
    "            \"dangerous_preceding\": dangerous_preceding,\n",
    "            \"examples\": ligature_examples,\n",
    "        }\n",
    "\n",
    "# Build output table\n",
    "context_rows = []\n",
    "for char in sorted(contextual_safety.keys()):\n",
    "    info = contextual_safety[char]\n",
    "    following = info[\"dangerous_following\"]\n",
    "    preceding = info[\"dangerous_preceding\"]\n",
    "\n",
    "    if following:\n",
    "        following_chars = \", \".join(f\"'{c}' ({utf8_hex(c)})\" for c in sorted(following))\n",
    "        safe_following = f\"NOT: {following_chars}\"\n",
    "    else:\n",
    "        safe_following = \"any\"\n",
    "\n",
    "    if preceding:\n",
    "        preceding_chars = \", \".join(f\"'{c}' ({utf8_hex(c)})\" for c in sorted(preceding))\n",
    "        safe_preceding = f\"NOT: {preceding_chars}\"\n",
    "    else:\n",
    "        safe_preceding = \"any\"\n",
    "\n",
    "    context_rows.append(\n",
    "        {\n",
    "            \"Char\": f\"'{char}'\",\n",
    "            \"Hex\": utf8_hex(char),\n",
    "            \"Safe following\": safe_following,\n",
    "            \"Safe preceding\": safe_preceding,\n",
    "            \"Examples\": \", \".join(info[\"examples\"]),\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(tabulate(context_rows, headers=\"keys\", tablefmt=\"github\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at this, if the needle contains a continuous sequence of `'b'`, `'c'`, `'d'`, `'e'`, `'g'`, `'m'`, `'o'`, `'p'`, `'q'`, `'r'`, `'u'`, `'v'`, `'x'`, `'z'` in any order or case, we can trivially match them using the simple SIMD snippet from above, as long as it doesn't contain `'a'`, `'f'`, `'h'`, `'i'`, `'j'`, `'k'`, `'l'`, `'n'`, `'s'`, `'t'`, `'w'`, or `'y'`.\n",
    "\n",
    "Moreover, there is a group of single-byte UTF-8 codepoints that don't participate in any folding mappings at all:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uninvolved_ascii = [\n",
    "    codepoint\n",
    "    for codepoint in range(UTF8_1BYTE_MAX + 1)\n",
    "    if codepoint not in ascii_targets and codepoint not in case_folds\n",
    "]\n",
    "print(f\"ASCII codepoints completely uninvolved in folding: {len(uninvolved_ascii)}\")\n",
    "\n",
    "control_characters = [codepoint for codepoint in uninvolved_ascii if codepoint < 32 or codepoint == 127]\n",
    "digits = [chr(codepoint) for codepoint in uninvolved_ascii if chr(codepoint).isdigit()]\n",
    "punctuation = [\n",
    "    chr(codepoint) for codepoint in uninvolved_ascii if 32 <= codepoint < 127 and not chr(codepoint).isalnum()\n",
    "]\n",
    "\n",
    "print(f\"Control characters: {len(control_characters)} (0x00-0x1F, 0x7F)\")\n",
    "print(f\"Digits: {''.join(digits)}\")\n",
    "print(f\"Punctuation/Symbols: {''.join(punctuation)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Safe Two-byte Folding Anchors\n",
    "\n",
    "The more interesting and challenging part is the 2-byte UTF-8 codepoints that map into either other single 2-byte codepoint or two 1-byte codepoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_byte_folds = {}\n",
    "for source_codepoint, target_codepoints in case_folds.items():\n",
    "    if UTF8_1BYTE_MAX < source_codepoint <= UTF8_2BYTE_MAX:  # 2-byte UTF-8 range\n",
    "        two_byte_folds[source_codepoint] = target_codepoints\n",
    "\n",
    "print(f\"2-byte UTF-8 codepoints with case folding: {len(two_byte_folds):,}\")\n",
    "print()\n",
    "\n",
    "# Categorize by target type\n",
    "folds_to_1byte = {}  # 2-byte → single 1-byte (e.g., some Latin letters)\n",
    "folds_to_2byte = {}  # 2-byte → single 2-byte (most common)\n",
    "folds_to_2x1byte = {}  # 2-byte → two 1-byte codepoints\n",
    "folds_to_other = {}  # Other patterns\n",
    "\n",
    "for source_codepoint, target_codepoints in two_byte_folds.items():\n",
    "    target_sizes = [\n",
    "        (\n",
    "            1\n",
    "            if codepoint <= UTF8_1BYTE_MAX\n",
    "            else 2 if codepoint <= UTF8_2BYTE_MAX else 3 if codepoint <= UTF8_3BYTE_MAX else 4\n",
    "        )\n",
    "        for codepoint in target_codepoints\n",
    "    ]\n",
    "\n",
    "    if len(target_codepoints) == 1:\n",
    "        if target_sizes[0] == 1:\n",
    "            folds_to_1byte[source_codepoint] = target_codepoints\n",
    "        elif target_sizes[0] == 2:\n",
    "            folds_to_2byte[source_codepoint] = target_codepoints\n",
    "        else:\n",
    "            folds_to_other[source_codepoint] = target_codepoints\n",
    "    elif len(target_codepoints) == 2 and all(size == 1 for size in target_sizes):\n",
    "        folds_to_2x1byte[source_codepoint] = target_codepoints\n",
    "    else:\n",
    "        folds_to_other[source_codepoint] = target_codepoints\n",
    "\n",
    "print(f\"Folding patterns for 2-byte UTF-8 sources:\")\n",
    "print(f\"  2-byte → 1-byte:     {len(folds_to_1byte):,}\")\n",
    "print(f\"  2-byte → 2-byte:     {len(folds_to_2byte):,}\")\n",
    "print(f\"  2-byte → 2x 1-byte:  {len(folds_to_2x1byte):,}\")\n",
    "print(f\"  Other patterns:      {len(folds_to_other):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of the 460 case folding rules for 2-byte UTF-8 sources, the vast majority (450) map to another 2-byte codepoint.\n",
    "The remaining 10 are special cases worth understanding.\n",
    "\n",
    "2-byte → 1-byte (1 case):\n",
    "\n",
    "- `'ſ'` (U+017F, LATIN SMALL LETTER LONG S) → `'s'` - historical long S folds to regular ASCII s\n",
    "\n",
    "2-byte → 2x 1-byte (1 case):\n",
    "\n",
    "- `'ß'` (U+00DF, LATIN SMALL LETTER SHARP S) → `\"ss\"` - German eszett expands to two ASCII characters\n",
    "\n",
    "Other patterns (8 cases) are the tricky edge cases that don't fit clean patterns:\n",
    "\n",
    "- `'İ'` (U+0130) → `'i'` + combining dot above (1-byte + 2-byte) - Turkish capital I with dot\n",
    "- `'ŉ'` (U+0149) → modifier apostrophe + `'n'` (2-byte + 1-byte) - deprecated character\n",
    "- `'ǰ'` (U+01F0) → `'j'` + combining caron (1-byte + 2-byte) - J with caron decomposes\n",
    "- `'Ⱥ'` (U+023A) → `'ⱥ'` (U+2C65) - 2-byte source maps to 3-byte target!\n",
    "- `'Ⱦ'` (U+023E) → `'ⱦ'` (U+2C66) - another 2-byte → 3-byte case\n",
    "- `'ΐ'` (U+0390) → ι + combining diaeresis + combining acute (3x 2-byte) - Greek with diacritics\n",
    "- `'ΰ'` (U+03B0) → υ + combining diaeresis + combining acute (3x 2-byte) - Greek with diacritics\n",
    "- `'և'` (U+0587) → ե + ւ (2x 2-byte) - Armenian ligature\n",
    "\n",
    "The `'Ⱥ'` and `'Ⱦ'` cases are particularly noteworthy: they are 2-byte UTF-8 sources that fold to 3-byte targets, meaning the folded form is longer than the original!\n",
    "Assuming the much larger search space, where possible, we want to group them into continuous to/from ranges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following table shows continuous ranges of 2-byte UTF-8 codepoints that fold to other 2-byte codepoints with a constant offset (e.g., uppercase → lowercase within the same script block):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_2byte = sorted(folds_to_2byte.items())\n",
    "\n",
    "ranges = []\n",
    "if sorted_2byte:\n",
    "    range_start = sorted_2byte[0][0]\n",
    "    range_offset = sorted_2byte[0][1][0] - sorted_2byte[0][0]\n",
    "    prev_source = sorted_2byte[0][0]\n",
    "\n",
    "    for source_codepoint, target_codepoints in sorted_2byte[1:]:\n",
    "        target_codepoint = target_codepoints[0]\n",
    "        current_offset = target_codepoint - source_codepoint\n",
    "\n",
    "        # Check if this continues the current range (consecutive source AND same offset)\n",
    "        if source_codepoint == prev_source + 1 and current_offset == range_offset:\n",
    "            prev_source = source_codepoint\n",
    "        else:\n",
    "            # End the current range and start a new one\n",
    "            ranges.append((range_start, prev_source, range_offset))\n",
    "            range_start = source_codepoint\n",
    "            range_offset = current_offset\n",
    "            prev_source = source_codepoint\n",
    "\n",
    "    # Don't forget the last range\n",
    "    ranges.append((range_start, prev_source, range_offset))\n",
    "\n",
    "print(f\"Found {len(ranges)} continuous ranges of 2-byte → 2-byte foldings\")\n",
    "print(f\"Ranges of length > 1: {sum(1 for r in ranges if r[1] - r[0] > 0)}\")\n",
    "print(f\"Single-codepoint 'ranges': {sum(1 for r in ranges if r[1] == r[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following table shows multi-codepoint ranges (length > 1) which are useful for SIMD optimization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build table with range information\n",
    "range_rows = []\n",
    "for start, end, offset in ranges:\n",
    "    length = end - start + 1\n",
    "    try:\n",
    "        start_char = chr(start)\n",
    "        end_char = chr(end)\n",
    "        target_start_char = chr(start + offset)\n",
    "        target_end_char = chr(end + offset)\n",
    "        script = unicodedata.name(start_char, \"\").split()[0] if length > 1 else \"\"\n",
    "    except (ValueError, OverflowError):\n",
    "        start_char = end_char = target_start_char = target_end_char = \"?\"\n",
    "        script = \"\"\n",
    "\n",
    "    range_rows.append(\n",
    "        {\n",
    "            \"Src Start\": f\"U+{start:04X} ({start_char})\",\n",
    "            \"Src Start Hex\": utf8_hex(start_char),\n",
    "            \"Src End\": f\"U+{end:04X} ({end_char})\",\n",
    "            \"Src End Hex\": utf8_hex(end_char),\n",
    "            \"Tgt Start\": f\"U+{start + offset:04X} ({target_start_char})\",\n",
    "            \"Tgt End\": f\"U+{end + offset:04X} ({target_end_char})\",\n",
    "            \"Len\": length,\n",
    "            \"Offset\": f\"+{offset}\" if offset > 0 else str(offset),\n",
    "            \"Script\": script,\n",
    "        }\n",
    "    )\n",
    "\n",
    "multi_ranges = [r for r in range_rows if r[\"Len\"] > 1]\n",
    "print(tabulate(multi_ranges, headers=\"keys\", tablefmt=\"github\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Three-byte UTF-8 Case Folding\n",
    "\n",
    "3-byte UTF-8 covers codepoints U+0800 to U+FFFF (2048 to 65535).\n",
    "This includes many scripts: Extended Greek, Cherokee, Georgian, and various symbol blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3-byte UTF-8 codepoints: U+0800 to U+FFFF (2048 to 65535)\n",
    "three_byte_folds = {}\n",
    "for source_codepoint, target_codepoints in case_folds.items():\n",
    "    if UTF8_2BYTE_MAX < source_codepoint <= UTF8_3BYTE_MAX:\n",
    "        three_byte_folds[source_codepoint] = target_codepoints\n",
    "\n",
    "print(f\"3-byte UTF-8 codepoints with case folding: {len(three_byte_folds):,}\")\n",
    "print()\n",
    "\n",
    "# Categorize by target pattern\n",
    "three_to_3byte = {}  # 3-byte → single 3-byte\n",
    "three_to_2byte = {}  # 3-byte → single 2-byte (shrinks!)\n",
    "three_to_1byte = {}  # 3-byte → 1-byte sequence\n",
    "three_to_other = {}  # Multi-codepoint or mixed\n",
    "\n",
    "for source_codepoint, target_codepoints in three_byte_folds.items():\n",
    "    target_sizes = [\n",
    "        1 if cp <= UTF8_1BYTE_MAX else 2 if cp <= UTF8_2BYTE_MAX else 3 if cp <= UTF8_3BYTE_MAX else 4\n",
    "        for cp in target_codepoints\n",
    "    ]\n",
    "\n",
    "    if len(target_codepoints) == 1:\n",
    "        if target_sizes[0] == 3:\n",
    "            three_to_3byte[source_codepoint] = target_codepoints\n",
    "        elif target_sizes[0] == 2:\n",
    "            three_to_2byte[source_codepoint] = target_codepoints\n",
    "        elif target_sizes[0] == 1:\n",
    "            three_to_1byte[source_codepoint] = target_codepoints\n",
    "        else:\n",
    "            three_to_other[source_codepoint] = target_codepoints\n",
    "    else:\n",
    "        three_to_other[source_codepoint] = target_codepoints\n",
    "\n",
    "print(f\"Folding patterns for 3-byte UTF-8 sources:\")\n",
    "print(f\"  3-byte → 3-byte:  {len(three_to_3byte):,}\")\n",
    "print(f\"  3-byte → 2-byte:  {len(three_to_2byte):,}\")\n",
    "print(f\"  3-byte → 1-byte:  {len(three_to_1byte):,}\")\n",
    "print(f\"  Other patterns:   {len(three_to_other):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following table shows continuous ranges of 3-byte UTF-8 codepoints that fold to other 3-byte codepoints:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find continuous ranges of 3-byte → 3-byte foldings\n",
    "sorted_3byte = sorted(three_to_3byte.items())\n",
    "\n",
    "ranges_3byte = []\n",
    "if sorted_3byte:\n",
    "    range_start = sorted_3byte[0][0]\n",
    "    range_offset = sorted_3byte[0][1][0] - sorted_3byte[0][0]\n",
    "    prev_source = sorted_3byte[0][0]\n",
    "\n",
    "    for source_codepoint, target_codepoints in sorted_3byte[1:]:\n",
    "        target_codepoint = target_codepoints[0]\n",
    "        current_offset = target_codepoint - source_codepoint\n",
    "\n",
    "        if source_codepoint == prev_source + 1 and current_offset == range_offset:\n",
    "            prev_source = source_codepoint\n",
    "        else:\n",
    "            ranges_3byte.append((range_start, prev_source, range_offset))\n",
    "            range_start = source_codepoint\n",
    "            range_offset = current_offset\n",
    "            prev_source = source_codepoint\n",
    "\n",
    "    ranges_3byte.append((range_start, prev_source, range_offset))\n",
    "\n",
    "print(f\"Found {len(ranges_3byte)} continuous ranges of 3-byte → 3-byte foldings\")\n",
    "print(f\"Ranges of length > 1: {sum(1 for r in ranges_3byte if r[1] - r[0] > 0)}\")\n",
    "print(f\"Single-codepoint 'ranges': {sum(1 for r in ranges_3byte if r[1] == r[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following table shows multi-codepoint ranges (length > 1) which are useful for SIMD optimization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build table\n",
    "range_rows_3byte = []\n",
    "for start, end, offset in ranges_3byte:\n",
    "    length = end - start + 1\n",
    "    try:\n",
    "        start_char = chr(start)\n",
    "        end_char = chr(end)\n",
    "        target_start_char = chr(start + offset)\n",
    "        target_end_char = chr(end + offset)\n",
    "        script = unicodedata.name(start_char, \"\").split()[0] if length > 1 else \"\"\n",
    "    except (ValueError, OverflowError):\n",
    "        start_char = end_char = target_start_char = target_end_char = \"?\"\n",
    "        script = \"\"\n",
    "\n",
    "    range_rows_3byte.append(\n",
    "        {\n",
    "            \"Src Start\": f\"U+{start:04X} ({start_char})\",\n",
    "            \"Src Start Hex\": utf8_hex(start_char),\n",
    "            \"Src End\": f\"U+{end:04X} ({end_char})\",\n",
    "            \"Src End Hex\": utf8_hex(end_char),\n",
    "            \"Tgt Start\": f\"U+{start + offset:04X} ({target_start_char})\",\n",
    "            \"Tgt End\": f\"U+{end + offset:04X} ({target_end_char})\",\n",
    "            \"Len\": length,\n",
    "            \"Offset\": f\"+{offset}\" if offset > 0 else str(offset),\n",
    "            \"Script\": script,\n",
    "        }\n",
    "    )\n",
    "\n",
    "multi_ranges_3byte = [r for r in range_rows_3byte if r[\"Len\"] > 1]\n",
    "print(tabulate(multi_ranges_3byte, headers=\"keys\", tablefmt=\"github\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Script-by-Script Analysis\n",
    "\n",
    "Now that we understand the general structure of Unicode case folding, let's dive into specific scripts.\n",
    "For each script, we'll answer these questions:\n",
    "\n",
    "1. **What UTF-8 byte patterns identify this script?** (Lead bytes)\n",
    "2. **Are there any multi-character expansions?** (Characters that become multiple codepoints)\n",
    "3. **What are the safe ranges for SIMD fast paths?**\n",
    "4. **What contextual safety rules apply?**\n",
    "\n",
    "We'll start with the \"easiest\" scripts (simple 1:1 mappings) and work toward the more complex ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cyrillic (U+0400-U+04FF)\n",
    "\n",
    "Cyrillic is the writing system for Russian, Ukrainian, Bulgarian, Serbian, and many other languages.\n",
    "It's one of the most SIMD-friendly scripts because:\n",
    "\n",
    "- **All case folding is 1:1** - no multi-character expansions\n",
    "- **Predictable offsets** - uppercase letters map to lowercase with fixed offsets\n",
    "- **Compact UTF-8** - all characters fit in 2 bytes (lead bytes 0xD0 and 0xD1)\n",
    "\n",
    "Let's verify this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cyrillic range: U+0400 to U+04FF (main block)\n",
    "CYRILLIC_START = 0x0400\n",
    "CYRILLIC_END = 0x04FF\n",
    "\n",
    "# Extract Cyrillic case folding rules\n",
    "cyrillic_folds = {}\n",
    "cyrillic_expansions = {}  # Multi-character expansions\n",
    "cyrillic_simple = {}      # Simple 1:1 mappings\n",
    "\n",
    "for source_codepoint, target_codepoints in case_folds.items():\n",
    "    if CYRILLIC_START <= source_codepoint <= CYRILLIC_END:\n",
    "        cyrillic_folds[source_codepoint] = target_codepoints\n",
    "        if len(target_codepoints) == 1:\n",
    "            cyrillic_simple[source_codepoint] = target_codepoints[0]\n",
    "        else:\n",
    "            cyrillic_expansions[source_codepoint] = target_codepoints\n",
    "\n",
    "print(f\"Cyrillic case folding rules: {len(cyrillic_folds)}\")\n",
    "print(f\"  Simple 1:1 mappings: {len(cyrillic_simple)}\")\n",
    "print(f\"  Multi-character expansions: {len(cyrillic_expansions)}\")\n",
    "\n",
    "if cyrillic_expansions:\n",
    "    print(\"\\n⚠️ Found expansions:\")\n",
    "    for source, targets in cyrillic_expansions.items():\n",
    "        print(f\"  '{chr(source)}' → {''.join(chr(t) for t in targets)}\")\n",
    "else:\n",
    "    print(\"\\n✓ No multi-character expansions! Perfect for SIMD.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see the actual mapping pattern.\n",
    "In Cyrillic, uppercase letters have predictable relationships to lowercase:\n",
    "\n",
    "- U+0410-U+042F (А-Я) → U+0430-U+044F (а-я) with offset +32\n",
    "- U+0400-U+040F (Ѐ-Џ) → U+0450-U+045F (ѐ-џ) with offset +80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show Cyrillic alphabet with case mappings\n",
    "print(\"Main Cyrillic alphabet (А-Я → а-я):\")\n",
    "print()\n",
    "\n",
    "rows = []\n",
    "for cp in range(0x0410, 0x0430):  # А to Я\n",
    "    upper = chr(cp)\n",
    "    lower = chr(cp + 32)  # Known offset\n",
    "    upper_utf8 = ' '.join(f'{b:02X}' for b in upper.encode('utf-8'))\n",
    "    lower_utf8 = ' '.join(f'{b:02X}' for b in lower.encode('utf-8'))\n",
    "    rows.append({\n",
    "        'Upper': upper,\n",
    "        'Lower': lower,\n",
    "        'Upper UTF-8': upper_utf8,\n",
    "        'Lower UTF-8': lower_utf8,\n",
    "        'Offset': '+32'\n",
    "    })\n",
    "\n",
    "print(tabulate(rows[:16], headers='keys', tablefmt='github'))  # First 16\n",
    "print(\"...\")\n",
    "print(tabulate(rows[-6:], headers='keys', tablefmt='github'))  # Last 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the UTF-8 pattern:\n",
    "- Lead byte 0xD0 covers U+0400-U+043F\n",
    "- Lead byte 0xD1 covers U+0440-U+04FF\n",
    "\n",
    "This means when we see `0xD0` or `0xD1` as a lead byte, we know we're in Cyrillic territory.\n",
    "The case folding is just arithmetic on the second byte!\n",
    "\n",
    "**SIMD Strategy for Cyrillic:**\n",
    "1. Detect lead bytes 0xD0/0xD1 in 64-byte chunks\n",
    "2. For uppercase ranges, add offset to second byte\n",
    "3. Handle lead byte transitions (some uppercase chars cross from 0xD0 to 0xD1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Greek (U+0370-U+03FF, U+1F00-U+1FFF)\n",
    "\n",
    "Greek is more interesting than Cyrillic because of historical features:\n",
    "\n",
    "- **Final sigma (ς)**: Greek uses different forms of sigma at word-end vs. middle\n",
    "- **Polytonic orthography**: Ancient/formal Greek uses multiple diacritics (accents, breathings)\n",
    "- **Iota subscript**: A small iota written beneath vowels in certain grammatical forms\n",
    "\n",
    "Basic Greek (U+0370-U+03FF) is 2-byte UTF-8 with lead bytes 0xCE/0xCF.\n",
    "Extended Greek (U+1F00-U+1FFF) is 3-byte UTF-8 for polytonic characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Greek ranges\n",
    "GREEK_BASIC_START = 0x0370\n",
    "GREEK_BASIC_END = 0x03FF\n",
    "GREEK_EXTENDED_START = 0x1F00\n",
    "GREEK_EXTENDED_END = 0x1FFF\n",
    "\n",
    "# Extract Greek case folding rules\n",
    "greek_folds = {}\n",
    "greek_expansions = {}\n",
    "greek_simple = {}\n",
    "\n",
    "for source_codepoint, target_codepoints in case_folds.items():\n",
    "    if (GREEK_BASIC_START <= source_codepoint <= GREEK_BASIC_END or \n",
    "        GREEK_EXTENDED_START <= source_codepoint <= GREEK_EXTENDED_END):\n",
    "        greek_folds[source_codepoint] = target_codepoints\n",
    "        if len(target_codepoints) == 1:\n",
    "            greek_simple[source_codepoint] = target_codepoints[0]\n",
    "        else:\n",
    "            greek_expansions[source_codepoint] = target_codepoints\n",
    "\n",
    "print(f\"Greek case folding rules: {len(greek_folds)}\")\n",
    "print(f\"  Basic Greek (U+0370-U+03FF): {sum(1 for cp in greek_folds if cp <= GREEK_BASIC_END)}\")\n",
    "print(f\"  Extended Greek (U+1F00-U+1FFF): {sum(1 for cp in greek_folds if cp >= GREEK_EXTENDED_START)}\")\n",
    "print()\n",
    "print(f\"Simple 1:1 mappings: {len(greek_simple)}\")\n",
    "print(f\"Multi-character expansions: {len(greek_expansions)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a lot of expansions!\n",
    "Let's understand why by looking at some examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Greek multi-character expansions (first 15):\")\n",
    "print()\n",
    "\n",
    "rows = []\n",
    "for source, targets in list(greek_expansions.items())[:15]:\n",
    "    source_char = chr(source)\n",
    "    target_str = ''.join(chr(t) for t in targets)\n",
    "    source_name = unicodedata.name(source_char, '?')\n",
    "    rows.append({\n",
    "        'Char': source_char,\n",
    "        'Codepoint': f'U+{source:04X}',\n",
    "        'Folds to': target_str,\n",
    "        'Length': len(targets),\n",
    "        'Name': source_name[:40] + '...' if len(source_name) > 40 else source_name\n",
    "    })\n",
    "\n",
    "print(tabulate(rows, headers='keys', tablefmt='github'))\n",
    "print(f\"\\n... and {len(greek_expansions) - 15} more\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The expansions all involve **iota subscript** (ypogegrammeni) or **diacritics**.\n",
    "When case-folded, these decompose:\n",
    "\n",
    "- `ᾼ` (alpha with ypogegrammeni) → `αι` (alpha + iota)\n",
    "- `ΐ` (iota with dialytika and tonos) → `ι` + combining marks\n",
    "\n",
    "But here's the good news: **basic Greek letters (Α-Ω) have simple 1:1 mappings!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the simple Greek alphabet\n",
    "print(\"Basic Greek alphabet (Α-Ω → α-ω):\")\n",
    "print()\n",
    "\n",
    "# Greek uppercase: U+0391-U+03A9 (with gap at U+03A2)\n",
    "# Greek lowercase: U+03B1-U+03C9\n",
    "greek_upper = 'ΑΒΓΔΕΖΗΘΙΚΛΜΝΞΟΠΡΣΤΥΦΧΨΩ'\n",
    "greek_lower = 'αβγδεζηθικλμνξοπρστυφχψω'\n",
    "\n",
    "rows = []\n",
    "for u, l in zip(greek_upper, greek_lower):\n",
    "    rows.append({\n",
    "        'Upper': u,\n",
    "        'Lower': l,\n",
    "        'Upper UTF-8': ' '.join(f'{b:02X}' for b in u.encode('utf-8')),\n",
    "        'Lower UTF-8': ' '.join(f'{b:02X}' for b in l.encode('utf-8'))\n",
    "    })\n",
    "\n",
    "print(tabulate(rows, headers='keys', tablefmt='github'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The Sigma Problem**\n",
    "\n",
    "Greek has three forms of sigma:\n",
    "- `Σ` (U+03A3) - uppercase sigma\n",
    "- `σ` (U+03C3) - lowercase sigma (middle of word)\n",
    "- `ς` (U+03C2) - lowercase final sigma (end of word)\n",
    "\n",
    "For case-insensitive matching, how does this work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigma case folding\n",
    "sigma_chars = {\n",
    "    0x03A3: ('Σ', 'CAPITAL'),\n",
    "    0x03C3: ('σ', 'SMALL'),\n",
    "    0x03C2: ('ς', 'SMALL FINAL')\n",
    "}\n",
    "\n",
    "print(\"Sigma variants and their case folding:\")\n",
    "print()\n",
    "\n",
    "for cp, (char, desc) in sigma_chars.items():\n",
    "    if cp in case_folds:\n",
    "        target = case_folds[cp][0]\n",
    "        target_char = chr(target)\n",
    "        print(f\"  {char} ({desc}, U+{cp:04X}) → {target_char} (U+{target:04X})\")\n",
    "    else:\n",
    "        print(f\"  {char} ({desc}, U+{cp:04X}) → (no folding rule, is target)\")\n",
    "\n",
    "print()\n",
    "print(\"✓ Both Σ and ς fold to σ!\")\n",
    "print(\"  This means σ is the 'anchor' - when searching for 'σ',\")\n",
    "print(\"  we match both 'Σ' and 'ς' automatically.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Armenian (U+0530-U+058F)\n",
    "\n",
    "Armenian is a bicameral script with 38 letters.\n",
    "It's nearly as SIMD-friendly as Cyrillic, with just one exception:\n",
    "the ligature **և** (ech-yiwn).\n",
    "\n",
    "UTF-8: 2-byte sequences with lead bytes 0xD4, 0xD5, 0xD6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Armenian range\n",
    "ARMENIAN_START = 0x0530\n",
    "ARMENIAN_END = 0x058F\n",
    "\n",
    "# Extract Armenian case folding rules\n",
    "armenian_folds = {}\n",
    "armenian_expansions = {}\n",
    "armenian_simple = {}\n",
    "\n",
    "for source_codepoint, target_codepoints in case_folds.items():\n",
    "    if ARMENIAN_START <= source_codepoint <= ARMENIAN_END:\n",
    "        armenian_folds[source_codepoint] = target_codepoints\n",
    "        if len(target_codepoints) == 1:\n",
    "            armenian_simple[source_codepoint] = target_codepoints[0]\n",
    "        else:\n",
    "            armenian_expansions[source_codepoint] = target_codepoints\n",
    "\n",
    "print(f\"Armenian case folding rules: {len(armenian_folds)}\")\n",
    "print(f\"  Simple 1:1 mappings: {len(armenian_simple)}\")\n",
    "print(f\"  Multi-character expansions: {len(armenian_expansions)}\")\n",
    "\n",
    "if armenian_expansions:\n",
    "    print(\"\\nThe expansion(s):\")\n",
    "    for source, targets in armenian_expansions.items():\n",
    "        source_char = chr(source)\n",
    "        target_str = ''.join(chr(t) for t in targets)\n",
    "        print(f\"  '{source_char}' (U+{source:04X}) → \\\"{target_str}\\\"\")\n",
    "        print(f\"  {unicodedata.name(source_char, '?')}\")\n",
    "        print(f\"  UTF-8: {utf8_hex(source_char)} → {utf8_hex(target_str)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Armenian ligature **և** is the only complication.\n",
    "When case-folding, it expands to **եdelays** (ech + yiwn).\n",
    "\n",
    "This is similar to German ß → ss, but less common in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Georgian (U+10A0-U+10FF, U+1C90-U+1CBF, U+2D00-U+2D2F)\n",
    "\n",
    "Georgian is fascinating because it has **three** historical writing systems:\n",
    "\n",
    "1. **Asomtavruli** (U+10A0-U+10C5) - ancient capitals, now uppercase\n",
    "2. **Nuskhuri** (U+2D00-U+2D2F) - medieval lowercase (rarely used)\n",
    "3. **Mkhedruli** (U+10D0-U+10FF) - modern lowercase (most common)\n",
    "4. **Mtavruli** (U+1C90-U+1CBF) - modern uppercase (added in Unicode 11.0)\n",
    "\n",
    "For most of its history, Georgian was **unicameral** (no case distinction).\n",
    "Uppercase/lowercase was only formalized recently!\n",
    "\n",
    "UTF-8: All Georgian is 3-byte (lead byte 0xE1 or 0xE2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Georgian ranges\n",
    "GEORGIAN_ASOMTAVRULI = (0x10A0, 0x10C5)  # Ancient capitals\n",
    "GEORGIAN_MKHEDRULI = (0x10D0, 0x10FF)    # Modern lowercase\n",
    "GEORGIAN_MTAVRULI = (0x1C90, 0x1CBF)     # Modern uppercase\n",
    "GEORGIAN_NUSKHURI = (0x2D00, 0x2D2F)     # Medieval lowercase\n",
    "\n",
    "# Extract Georgian case folding rules\n",
    "georgian_folds = {}\n",
    "for source_codepoint, target_codepoints in case_folds.items():\n",
    "    if any(start <= source_codepoint <= end for start, end in \n",
    "           [GEORGIAN_ASOMTAVRULI, GEORGIAN_MKHEDRULI, GEORGIAN_MTAVRULI, GEORGIAN_NUSKHURI]):\n",
    "        georgian_folds[source_codepoint] = target_codepoints\n",
    "\n",
    "print(f\"Georgian case folding rules: {len(georgian_folds)}\")\n",
    "print()\n",
    "\n",
    "# Check for expansions\n",
    "georgian_expansions = {k: v for k, v in georgian_folds.items() if len(v) > 1}\n",
    "print(f\"Multi-character expansions: {len(georgian_expansions)}\")\n",
    "\n",
    "if not georgian_expansions:\n",
    "    print(\"✓ All Georgian case folding is simple 1:1 mappings!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show Georgian alphabet examples\n",
    "print(\"Georgian case mapping examples:\")\n",
    "print()\n",
    "\n",
    "# Mtavruli (modern uppercase) → Mkhedruli (modern lowercase)\n",
    "print(\"Mtavruli (modern uppercase) → Mkhedruli (modern lowercase):\")\n",
    "rows = []\n",
    "for i, cp in enumerate(range(0x1C90, 0x1C90 + 8)):  # First 8\n",
    "    if cp in georgian_folds:\n",
    "        upper = chr(cp)\n",
    "        lower = chr(georgian_folds[cp][0])\n",
    "        rows.append({\n",
    "            'Upper': upper,\n",
    "            'Lower': lower,\n",
    "            'Upper UTF-8': utf8_hex(upper),\n",
    "            'Lower UTF-8': utf8_hex(lower)\n",
    "        })\n",
    "\n",
    "print(tabulate(rows, headers='keys', tablefmt='github'))\n",
    "print()\n",
    "print(\"Notice: Georgian letters are გ ბ ა - they look quite different from Latin!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cherokee (U+13A0-U+13FF, U+AB70-U+ABBF)\n",
    "\n",
    "Cherokee is **unique** among writing systems:\n",
    "its lowercase letters fold **to uppercase** (opposite of every other script!).\n",
    "\n",
    "This happened because Cherokee lowercase was added to Unicode later,\n",
    "after the uppercase forms were already established.\n",
    "\n",
    "- Uppercase: U+13A0-U+13F5 (original Cherokee syllabary)\n",
    "- Lowercase: U+AB70-U+ABBF (added in Unicode 8.0, called \"Cherokee Supplement\")\n",
    "\n",
    "UTF-8: 3-byte sequences (lead bytes 0xE1 and 0xEA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cherokee ranges\n",
    "CHEROKEE_UPPER = (0x13A0, 0x13F5)\n",
    "CHEROKEE_LOWER = (0xAB70, 0xABBF)\n",
    "\n",
    "# Extract Cherokee case folding\n",
    "cherokee_folds = {}\n",
    "for source_codepoint, target_codepoints in case_folds.items():\n",
    "    if (CHEROKEE_UPPER[0] <= source_codepoint <= CHEROKEE_UPPER[1] or\n",
    "        CHEROKEE_LOWER[0] <= source_codepoint <= CHEROKEE_LOWER[1]):\n",
    "        cherokee_folds[source_codepoint] = target_codepoints\n",
    "\n",
    "print(f\"Cherokee case folding rules: {len(cherokee_folds)}\")\n",
    "print()\n",
    "\n",
    "# Show examples\n",
    "print(\"Cherokee lowercase → uppercase (unusual!):\")\n",
    "rows = []\n",
    "for cp in range(0xAB70, 0xAB70 + 6):  # First 6\n",
    "    if cp in cherokee_folds:\n",
    "        lower = chr(cp)\n",
    "        upper = chr(cherokee_folds[cp][0])\n",
    "        rows.append({\n",
    "            'Lower': lower,\n",
    "            'Upper (fold target)': upper,\n",
    "            'Lower UTF-8': utf8_hex(lower),\n",
    "            'Upper UTF-8': utf8_hex(upper)\n",
    "        })\n",
    "\n",
    "print(tabulate(rows, headers='keys', tablefmt='github'))\n",
    "print()\n",
    "print(\"Notice: The 'fold target' is UPPERCASE, not lowercase!\")\n",
    "print(\"Cherokee is the only script where case folding goes lowercase → uppercase.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unicameral Scripts (No Case Distinction)\n",
    "\n",
    "Many of the world's writing systems don't have uppercase/lowercase.\n",
    "These \"unicameral\" scripts are **caseless** - they have no case folding rules at all.\n",
    "\n",
    "This is great for SIMD optimization!\n",
    "When searching in caseless text, we can use fast binary comparison\n",
    "instead of case folding.\n",
    "\n",
    "Major unicameral scripts include:\n",
    "- **CJK** (Chinese, Japanese Kanji, Korean Hanja)\n",
    "- **Arabic** and **Hebrew**\n",
    "- **Thai**, **Devanagari** (Hindi), and other Indic scripts\n",
    "- **Japanese Hiragana/Katakana**\n",
    "- **Korean Hangul**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that these scripts have no case folding\n",
    "unicameral_ranges = {\n",
    "    'CJK Unified': (0x4E00, 0x9FFF),\n",
    "    'Hiragana': (0x3040, 0x309F),\n",
    "    'Katakana': (0x30A0, 0x30FF),\n",
    "    'Hangul Syllables': (0xAC00, 0xD7AF),\n",
    "    'Arabic': (0x0600, 0x06FF),\n",
    "    'Hebrew': (0x0590, 0x05FF),\n",
    "    'Thai': (0x0E00, 0x0E7F),\n",
    "    'Devanagari': (0x0900, 0x097F),\n",
    "}\n",
    "\n",
    "print(\"Unicameral script verification:\")\n",
    "print()\n",
    "\n",
    "rows = []\n",
    "for name, (start, end) in unicameral_ranges.items():\n",
    "    # Count codepoints with case folding rules\n",
    "    with_folding = sum(1 for cp in range(start, end + 1) if cp in case_folds)\n",
    "    total = end - start + 1\n",
    "    \n",
    "    # Get sample characters\n",
    "    samples = ''.join(chr(cp) for cp in [start, start+1, start+2] \n",
    "                      if unicodedata.category(chr(cp))[0] == 'L')[:3]\n",
    "    \n",
    "    rows.append({\n",
    "        'Script': name,\n",
    "        'Range': f'U+{start:04X}-U+{end:04X}',\n",
    "        'Sample': samples,\n",
    "        'Case Folding Rules': with_folding,\n",
    "        'Caseless?': '✓' if with_folding == 0 else f'✗ ({with_folding})'\n",
    "    })\n",
    "\n",
    "print(tabulate(rows, headers='keys', tablefmt='github'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For case-insensitive search with unicameral needles,\n",
    "we can bypass case folding entirely and use fast binary `memcmp`.\n",
    "\n",
    "The function `sz_utf8_is_fully_caseless_` in StringZilla detects this:\n",
    "- Scan the needle for bicameral characters\n",
    "- If none found, use fast path (binary search)\n",
    "- If found, use case-folded search (slower)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical Search Examples\n",
    "\n",
    "Let's see how all this theory applies to real searches.\n",
    "For each example, we'll show:\n",
    "1. The search needle\n",
    "2. What it case-folds to\n",
    "3. What optimization path StringZilla uses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_needle(needle):\n",
    "    \"\"\"Analyze a search needle for case-folding behavior.\"\"\"\n",
    "    print(f\"Needle: '{needle}'\")\n",
    "    print(f\"UTF-8:  {utf8_hex(needle)}\")\n",
    "    print()\n",
    "    \n",
    "    # Case fold each character\n",
    "    folded_chars = []\n",
    "    has_bicameral = False\n",
    "    has_expansion = False\n",
    "    \n",
    "    for char in needle:\n",
    "        cp = ord(char)\n",
    "        if cp in case_folds:\n",
    "            targets = case_folds[cp]\n",
    "            folded = ''.join(chr(t) for t in targets)\n",
    "            folded_chars.append(folded)\n",
    "            has_bicameral = True\n",
    "            if len(targets) > 1:\n",
    "                has_expansion = True\n",
    "                print(f\"  '{char}' (U+{cp:04X}) → \\\"{folded}\\\" (expansion!)\")\n",
    "            else:\n",
    "                print(f\"  '{char}' (U+{cp:04X}) → '{folded}'\")\n",
    "        else:\n",
    "            folded_chars.append(char)\n",
    "            # Check if it's a lowercase that doesn't fold\n",
    "            cat = unicodedata.category(char)\n",
    "            if cat == 'Ll':  # Lowercase letter\n",
    "                has_bicameral = True\n",
    "                print(f\"  '{char}' (U+{cp:04X}) → '{char}' (lowercase, no change)\")\n",
    "    \n",
    "    folded_str = ''.join(folded_chars)\n",
    "    print()\n",
    "    print(f\"Folded: '{folded_str}'\")\n",
    "    print(f\"UTF-8:  {utf8_hex(folded_str)}\")\n",
    "    print()\n",
    "    \n",
    "    if not has_bicameral:\n",
    "        print(\"Optimization: FAST PATH (caseless needle, binary search)\")\n",
    "    elif has_expansion:\n",
    "        print(\"Optimization: SLOW PATH (has expansions, must case-fold)\")\n",
    "    else:\n",
    "        print(\"Optimization: SIMD FAST PATH (simple 1:1 folding)\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Pure ASCII\n",
    "print(\"=\"*60)\n",
    "print(\"Example 1: ASCII needle\")\n",
    "print(\"=\"*60)\n",
    "analyze_needle(\"Hello World\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Cyrillic\n",
    "print(\"=\"*60)\n",
    "print(\"Example 2: Cyrillic needle (Russian 'Hello')\")\n",
    "print(\"=\"*60)\n",
    "analyze_needle(\"ПРИВЕТ\")  # Russian for 'hello'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: German with eszett\n",
    "print(\"=\"*60)\n",
    "print(\"Example 3: German with ß (expands to 'ss')\")\n",
    "print(\"=\"*60)\n",
    "analyze_needle(\"straße\")  # German for 'street'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 4: CJK (caseless)\n",
    "print(\"=\"*60)\n",
    "print(\"Example 4: Chinese (caseless, fast path!)\")\n",
    "print(\"=\"*60)\n",
    "analyze_needle(\"中文\")  # Chinese for 'Chinese'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 5: Greek with final sigma\n",
    "print(\"=\"*60)\n",
    "print(\"Example 5: Greek with final sigma\")\n",
    "print(\"=\"*60)\n",
    "analyze_needle(\"ΚΌΣΜΟΣ\")  # Greek for 'world' (uses regular sigma)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "StringZilla",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
